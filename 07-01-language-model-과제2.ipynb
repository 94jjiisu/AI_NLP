{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"07-01-language-model-과제2.ipynb","provenance":[],"collapsed_sections":["6dfJPT-2XMTB","JHkHg6XAXoyK","XwriCkq_R1Lc","3myBX8hNEH1u","jv0OPyaKSpjw","L2vJmIBt3-ci","_dBAGBHE3-ck","MFWK8EvH3-cp","g2rxLFrL3-cq","rfTrHA3v3-cr"],"authorship_tag":"ABX9TyPFWlYf+Ccqsai6qAQf5dCH"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"6dfJPT-2XMTB"},"source":["# Install"]},{"cell_type":"code","metadata":{"id":"a193aGJWVaqb"},"source":["!pip install sentencepiece"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JHkHg6XAXoyK"},"source":["# Evn"]},{"cell_type":"code","metadata":{"id":"WkYXFwcBXJDG"},"source":["import os\n","import random\n","import shutil\n","import json\n","import zipfile\n","import math\n","import copy\n","import collections\n","import re\n","\n","import matplotlib.pyplot as plt\n","import pandas as pd\n","import numpy as np\n","import sentencepiece as spm\n","import tensorflow as tf\n","import tensorflow.keras.backend as K\n","\n","from tqdm.notebook import tqdm"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nvjyruUlXtlR"},"source":["# random seed initialize\n","random_seed = 1234\n","random.seed(random_seed)\n","np.random.seed(random_seed)\n","tf.random.set_seed(random_seed)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BC3fXkhdYcYt"},"source":["!nvidia-smi"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xVRdxYReYeQj"},"source":["# google drive mount\n","from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"byCIiLJBbFHh"},"source":["# data dir\n","data_dir = '/content/drive/MyDrive/Data/nlp'\n","os.listdir(data_dir)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_Ru56YS3-SME"},"source":["# korean wiki dir\n","kowiki_dir = os.path.join(data_dir, 'kowiki')\n","if not os.path.exists(kowiki_dir):\n","    os.makedirs(kowiki_dir)\n","os.listdir(kowiki_dir)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XwriCkq_R1Lc"},"source":["# Vocabulary & config"]},{"cell_type":"code","metadata":{"id":"2H0BLydCb7lg"},"source":["# vocab loading\n","vocab = spm.SentencePieceProcessor()\n","vocab.load(os.path.join(data_dir, 'ko_32000.model'))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0ETZ19flTRmt"},"source":["n_vocab = len(vocab)  # number of vocabulary\n","n_seq = 256  # number of sequence\n","d_model = 256  # dimension of model"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3myBX8hNEH1u"},"source":["# 모델링"]},{"cell_type":"code","metadata":{"id":"SobekkcjEX_P"},"source":["def build_model(n_vocab, d_model, n_seq):\n","    \"\"\"\n","    문장 유사도 비교 모델\n","    :param n_vocab: vocabulary 단어 수\n","    :param d_model: 단어를 의미하는 벡터의 차원 수\n","    :param n_seq: 문장 길이 (단어 수)\n","    \"\"\"\n","    inputs = tf.keras.layers.Input((n_seq,))  # (bs, n_seq)\n","    # 입력 단어를 vector로 변환\n","    embedding = tf.keras.layers.Embedding(n_vocab, d_model)\n","    hidden = embedding(inputs)  # (bs, n_seq, d_model)\n","    # LSTM\n","    lstm = tf.keras.layers.LSTM(units=d_model * 2, return_sequences=True)\n","    hidden = lstm(hidden)  # (bs, n_seq, d_model * 2)\n","    # 다음단어 확률 분포\n","    dense = tf.keras.layers.Dense(n_vocab, activation=tf.nn.softmax)\n","    outputs = dense(hidden)\n","    # 학습할 모델 선언\n","    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n","    return model"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jv0OPyaKSpjw"},"source":["# Preprocessing\n"]},{"cell_type":"code","metadata":{"id":"E13I0Xr_kMX-"},"source":["# 파일 내용 확인\n","with zipfile.ZipFile(os.path.join(kowiki_dir, 'kowiki.txt.zip')) as z:\n","    with z.open('kowiki.txt') as f:\n","        for i, line in enumerate(f):\n","            line = line.decode('utf-8').strip()\n","            print(line)\n","            if i >= 100:\n","                break"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"98u39IsbSoBk"},"source":["# 파일 내용 확인 (주제단위)\n","with zipfile.ZipFile(os.path.join(kowiki_dir, 'kowiki.txt.zip')) as z:\n","    with z.open('kowiki.txt') as f:\n","        doc = []\n","        for i, line in enumerate(f):\n","            line = line.decode('utf-8').strip()\n","            if len(line) == 0:\n","                if len(doc) > 0:\n","                    break\n","            else:\n","                doc.append(line)\n","doc"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EKoCzDHNSy3r"},"source":["def create_train_instance(vocab, n_seq, doc):\n","    \"\"\"\n","    create train instance\n","    :param vocab: vocabulary object\n","    :param n_seq: sequece number\n","    :param doc: wiki document\n","    :return: train instance list\n","    \"\"\"\n","    n_max = n_seq - 1\n","    instance_list = []\n","\n","    chunk = []\n","    chunk_len = 0\n","    for i, line in enumerate(doc):\n","        tokens = vocab.encode_as_pieces(line)\n","        chunk.append(tokens)\n","        chunk_len += len(tokens)\n","        if n_max <= chunk_len or i >= len(doc) -1:\n","            # print()\n","            # print(chunk_len, chunk)\n","            instance = []\n","            for tokens in chunk:\n","                instance.extend(tokens)\n","            # print(len(instance), instance)\n","            instance = instance[:n_max]\n","            # print(len(instance), instance)\n","            instance_list.append(instance)\n","            chunk = []\n","            chunk_len = 0\n","\n","    return instance_list"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UNNBJKa_Abb7"},"source":["# instance 동작 확인\n","instance_list = create_train_instance(vocab, n_seq, doc)\n","for instance in instance_list:\n","    print(len(instance), instance)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"L7ZWtd8NqE0Z"},"source":["# instance를 json 형태로 저장하는 함수\n","def save_instance(vocab, n_seq, doc, o_f):\n","    instance_list = create_train_instance(vocab, n_seq, doc)\n","    for instance in instance_list:\n","        o_f.write(json.dumps({'token': instance}, ensure_ascii=False))\n","        o_f.write('\\n')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lAZD9iR6_oO_"},"source":["# 전체 문서에 대한 instance 생성\n","with open(os.path.join(kowiki_dir, 'kowiki_lm.json'), 'w') as o_f:\n","    with zipfile.ZipFile(os.path.join(kowiki_dir, 'kowiki.txt.zip')) as z:\n","        with z.open('kowiki.txt') as f:\n","            doc = []\n","            for i, line in enumerate(tqdm(f)):\n","                line = line.decode('utf-8').strip()\n","                if len(line) == 0:\n","                    if len(doc) > 0:\n","                        save_instance(vocab, n_seq, doc, o_f)\n","                        doc = []\n","                else:\n","                    doc.append(line)\n","            if len(doc) > 0:\n","                save_instance(vocab, n_seq, doc, o_f)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"i3joZ7O1r93K"},"source":["# 파일 라인수 확인\n","n_line = 0\n","with open(os.path.join(kowiki_dir, 'kowiki_lm.json')) as f:\n","    for line in f:\n","        n_line += 1\n","        if n_line <= 10:\n","            print(line)\n","n_line"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"L2vJmIBt3-ci"},"source":["# All Data Project"]},{"cell_type":"markdown","metadata":{"id":"_dBAGBHE3-ck"},"source":["## Data\n"]},{"cell_type":"code","metadata":{"id":"X7Jbbcdm3-ck"},"source":["def load_data(vocab, n_seq):\n","    \"\"\"\n","    Language Model 학습 데이터 생성\n","    :param vocab: vocabulary object\n","    :param n_seq: number of sequence\n","    :return inputs_1: input data 1\n","    :return inputs_2: input data 2\n","    :return labels: label data\n","    \"\"\"\n","    # line 수 조회\n","    n_line = 0\n","    with open(os.path.join(kowiki_dir, 'kowiki_lm.json')) as f:\n","        for line in f:\n","            n_line += 1\n","    # 최대 100,000개 데이터\n","    n_data = min(n_line, 100000)\n","    # 빈 데이터 생성\n","    inputs = np.zeros((n_data, n_seq)).astype(np.int32)\n","    labels = np.zeros((n_data, n_seq)).astype(np.int32)\n","\n","    with open(os.path.join(kowiki_dir, 'kowiki_lm.json')) as f:\n","        for i, line in enumerate(tqdm(f, total=n_data)):\n","            if i >= n_data:\n","                break\n","            data = json.loads(line)\n","            token_id = [vocab.piece_to_id(p) for p in data['token']]\n","            # input id\n","            input_id = [vocab.bos_id()] + token_id\n","            input_id += [0] * (n_seq - len(input_id))\n","            # label id\n","            label_id = token_id + [vocab.eos_id()]\n","            label_id += [0] * (n_seq - len(label_id))\n","            # 값 저장\n","            inputs[i] = input_id\n","            labels[i] = label_id\n","\n","    return inputs, labels"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"w3P_qk633-cl"},"source":["# train data 생성\n","train_inputs, train_labels = load_data(vocab, n_seq)\n","train_inputs, train_labels"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MFWK8EvH3-cp"},"source":["## Loss & Acc"]},{"cell_type":"code","metadata":{"id":"3cpcM1MW3-cp"},"source":["def lm_loss(y_true, y_pred):\n","    \"\"\"\n","    pad 부분을 제외하고 loss를 계산하는 함수\n","    :param y_true: 정답\n","    :param y_pred: 예측 값\n","    :retrun loss: pad 부분이 제외된 loss 값\n","    \"\"\"\n","    loss = tf.keras.losses.SparseCategoricalCrossentropy(reduction=tf.keras.losses.Reduction.NONE)(y_true, y_pred)\n","    mask = tf.not_equal(y_true, 0)\n","    mask = tf.cast(mask, tf.float32)\n","    loss *= mask\n","    return loss"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Xt7wUUf53-cp"},"source":["def lm_acc(y_true, y_pred):\n","    \"\"\"\n","    pad 부분을 제외하고 accuracy를 계산하는 함수\n","    :param y_true: 정답\n","    :param y_pred: 예측 값\n","    :retrun loss: pad 부분이 제외된 accuracy 값\n","    \"\"\"\n","    y_true = tf.cast(y_true, tf.float32)\n","    y_pred_class = tf.cast(tf.argmax(y_pred, axis=-1), tf.float32)\n","    matches = tf.cast(tf.equal(y_true, y_pred_class), tf.float32)\n","    mask = tf.not_equal(y_true, 0)\n","    mask = tf.cast(mask, tf.float32)\n","    matches *= mask\n","    accuracy = tf.reduce_sum(matches) / tf.maximum(tf.reduce_sum(mask), 1)\n","    return accuracy"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"g2rxLFrL3-cq"},"source":["## 학습"]},{"cell_type":"code","metadata":{"id":"E45XLFFC3-cq"},"source":["# 모델 loss, optimizer, metric 정의\n","model.compile(loss=lm_loss, optimizer='adam', metrics=[lm_acc])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Js_YBU2l3-cq"},"source":["# early stopping\n","early_stopping = tf.keras.callbacks.EarlyStopping(monitor='lm_acc', patience=50)\n","# save weights callback\n","save_weights = tf.keras.callbacks.ModelCheckpoint(os.path.join(kowiki_dir, 'lm.hdf5'),\n","                                                  monitor='lm_acc',\n","                                                  verbose=1,\n","                                                  save_best_only=True,\n","                                                  mode=\"max\",\n","                                                  save_freq=\"epoch\",\n","                                                  save_weights_only=True)\n","# csv logger\n","csv_logger = tf.keras.callbacks.CSVLogger(os.path.join(kowiki_dir, 'lm.csv'))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mQqvywUG3-cq"},"source":["# 모델 학습\n","history = model.fit(train_inputs,\n","                    train_labels,\n","                    epochs=2,\n","                    batch_size=64,\n","                    callbacks=[early_stopping, save_weights, csv_logger])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EttHWg_w3-cr"},"source":["plt.figure(figsize=(12, 4))\n","\n","plt.subplot(1, 2, 1)\n","plt.plot(history.history['loss'], 'b-', label='loss')\n","plt.xlabel('Epoch')\n","plt.legend()\n","\n","plt.subplot(1, 2, 2)\n","plt.plot(history.history['lm_acc'], 'g-', label='acc')\n","plt.xlabel('Epoch')\n","plt.legend()\n","\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rfTrHA3v3-cr"},"source":["## Inference"]},{"cell_type":"code","metadata":{"id":"DSh7XdUg3-cr"},"source":["# 모델 생성\n","model = build_model(len(vocab), d_model, n_seq)\n","# train weight로 초기화\n","model.load_weights(os.path.join(kowiki_dir, 'lm.hdf5'))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nDtLZD4K3-cr"},"source":["def do_next(vocab, model, n_seq, string):\n","    \"\"\"\n","    다음단어 예측\n","    :param vocab: vocab\n","    :param model: model\n","    :param n_seq: number of seqence\n","    :param string: inpust string\n","    \"\"\"\n","    n_max = n_seq - 1\n","    \n","    tokens = vocab.encode_as_pieces(string)\n","    start_idx = len(tokens)\n","    token_id = [vocab.piece_to_id(p) for p in tokens][:n_max]\n","    token_id = [vocab.bos_id()] + token_id\n","    token_id += [0] * (n_seq - len(token_id))\n","    assert len(token_id) == n_seq\n","\n","    result = model.predict(np.array([token_id]))\n","    prob = result[0][start_idx]\n","    max_args = np.argsort(prob)[-10:]\n","    max_args = list(max_args)\n","    max_args.reverse()\n","\n","    next_prob = []\n","    for i in max_args:\n","        w = vocab.id_to_piece(int(i))\n","        p = prob[i]\n","        next_prob.append((w, p))\n","    return next_prob"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-_QF5gIm3-cr"},"source":["while True:\n","    string = input('시작 문장 > ')\n","    string = string.strip()\n","    if len(string) == 0:\n","        break\n","    next_prob = do_next(vocab, model, n_seq, string)\n","    for w, p in next_prob:\n","        print(f'{w}: {p}')\n","    print()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PJ2JSeaK3-cr"},"source":["def do_generate(vocab, model, n_seq, string):\n","    \"\"\"\n","    문장생성\n","    :param vocab: vocab\n","    :param model: model\n","    :param n_seq: number of seqence\n","    :param string: inpust string\n","    \"\"\"\n","    n_max = n_seq - 1\n","    tokens = vocab.encode_as_pieces(string)\n","    start_idx = len(tokens)\n","    token_id = [vocab.piece_to_id(p) for p in tokens][:n_max]\n","    token_id = [vocab.bos_id()] + token_id\n","    token_id += [0] * (n_seq - len(token_id))\n","    assert len(token_id) == n_seq\n","\n","    for _ in range(start_idx, n_seq - 1):\n","        outputs = model.predict(np.array([token_id]))\n","        prob = outputs[0][start_idx]\n","        word_id = int(np.random.choice(len(vocab), 1, p=prob)[0])\n","        # word_id = np.argmax(prob)\n","        if word_id == vocab.eos_id():\n","            break\n","        token_id[start_idx + 1] = word_id\n","        start_idx += 1\n","    predict_id = token_id[1:start_idx + 1]\n","    predict_str = vocab.decode_ids(predict_id)\n","    return predict_str"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ts-fnAO73-cs"},"source":["while True:\n","    string = input('시작 문장 > ')\n","    string = string.strip()\n","    if len(string) == 0:\n","        break\n","    predict_str = do_generate(vocab, model, n_seq, string)\n","    print(predict_str)"],"execution_count":null,"outputs":[]}]}