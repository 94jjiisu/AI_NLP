{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"09-01-attention-강의.ipynb","provenance":[],"collapsed_sections":["6dfJPT-2XMTB","JHkHg6XAXoyK","tPfBcykRf6TH","D-bjMMvkBRLU","XwriCkq_R1Lc","kTgzpx2Wl6uv","gcG7zBAr_R_4","jv0OPyaKSpjw","IFTsdA3kE2h-","PQvKAxTbaEpt","wZSehzviqJW3","x0ZAlao5qJW4","ByyA-3bbqJW6","2lWovHbAqJW7"],"authorship_tag":"ABX9TyPyMlUvEe+bXzXbsBTh8iFy"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"6dfJPT-2XMTB"},"source":["# Install"]},{"cell_type":"code","metadata":{"id":"a193aGJWVaqb"},"source":["!pip install sentencepiece"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JHkHg6XAXoyK"},"source":["# Evn"]},{"cell_type":"code","metadata":{"id":"WkYXFwcBXJDG"},"source":["import os\n","import random\n","import shutil\n","import json\n","import zipfile\n","import math\n","import copy\n","import collections\n","import re\n","\n","import matplotlib.pyplot as plt\n","import pandas as pd\n","import numpy as np\n","import sentencepiece as spm\n","import tensorflow as tf\n","import tensorflow.keras.backend as KK\n","\n","from tqdm.notebook import tqdm"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nvjyruUlXtlR"},"source":["# random seed initialize\n","random_seed = 1234\n","random.seed(random_seed)\n","np.random.seed(random_seed)\n","tf.random.set_seed(random_seed)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BC3fXkhdYcYt"},"source":["!nvidia-smi"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xVRdxYReYeQj"},"source":["# google drive mount\n","from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"byCIiLJBbFHh"},"source":["# data dir\n","data_dir = '/content/drive/MyDrive/Data/nlp'\n","os.listdir(data_dir)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_Ru56YS3-SME"},"source":["# songys chatbot dir\n","songys_dir = os.path.join(data_dir, 'songys')\n","if not os.path.exists(songys_dir):\n","    os.makedirs(songys_dir)\n","os.listdir(songys_dir)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tPfBcykRf6TH"},"source":["# Attention"]},{"cell_type":"code","metadata":{"id":"GBCgqohCf-kO"},"source":["# 입력 문장\n","sentences = [\n","    ['나는 오늘 기분이 좋아', '네가 좋으니 나도 좋아'],\n","    ['나는 오늘 행복해', '나도 행복하다'],\n","]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"e0C4BfF9e7yg"},"source":["# 각 문장을 띄어쓰기 단위로 분할\n","words = []\n","for pair in sentences:\n","    for sentence in pair:\n","        words.extend(sentence.split())\n","\n","# 중복 단어 제거\n","words = list(dict.fromkeys(words))\n","\n","# 각 단어별 고유한 번호 부여\n","word_to_id = {'[PAD]': 0, '[UNK]': 1, '[BOS]': 2, '[EOS]': 3}\n","for word in words:\n","    word_to_id[word] = len(word_to_id)\n","\n","# 각 숫자별 단어 부여\n","id_to_word = {_id:word for word, _id in word_to_id.items()}\n","\n","word_to_id, id_to_word"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"t6eZMVpZfOQp"},"source":["# Question과 Answer를 숫자료\n","question_list, answer_list = [], []\n","\n","for pair in sentences:\n","    question_list.append([word_to_id[word] for word in pair[0].split()])\n","    answer_list.append([word_to_id[word] for word in pair[1].split()])\n","\n","# 학습용 입력 데이터 생성\n","train_enc_inputs, train_dec_inputs, train_labels = [], [], []\n","\n","for question, answer in zip(question_list, answer_list):\n","    train_enc_inputs.append(question)\n","    train_dec_inputs.append([word_to_id['[BOS]']] + answer)\n","    train_labels.append(answer + [word_to_id['[EOS]']])\n","\n","# Encoder 입력의 길이를 모두 동일하게 변경 (최대길이 4)\n","for row in train_enc_inputs:\n","    row += [0] * (4 - len(row))\n","\n","# Decoder 입력의 길이를 모두 동일하게 변경 (최대길이 5)\n","for row in train_dec_inputs:\n","    row += [0] * (5 - len(row))\n","\n","# 정답의 길이를 모두 동일하게 변경 (최대길이 5)\n","for row in train_labels:\n","    row += [0] * (5 - len(row))\n","\n","# numpy array로 변환\n","train_enc_inputs = np.array(train_enc_inputs)\n","train_dec_inputs = np.array(train_dec_inputs)\n","train_labels = np.array(train_labels)\n","\n","train_enc_inputs, train_dec_inputs, train_labels"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BljdfhNYfae4"},"source":["# random embed weight\n","embed_weight = np.random.randint(-9, 9, (len(id_to_word), 5)) / 10\n","\n","# embedding 생성\n","embedding = tf.keras.layers.Embedding(len(id_to_word), 5, weights=[embed_weight])\n","\n","# word embedding\n","hidden_enc = embedding(train_enc_inputs)  # (bs, n_enc_seq, 5)\n","hidden_dec = embedding(train_dec_inputs)  # (bs, n_dec_seq, 5)\n","hidden_enc, hidden_dec"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8SH71jMNfg-b"},"source":["# Q, K, V 선언"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Uqu9CiA_fpPK"},"source":["# Q[0][0] & K[0][0]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UzhoY1t7hiCt"},"source":["# Q[0][0] & K[0][1]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"80ZhiP3nh9kF"},"source":["# Q[0][0] & K[0][2]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"N8MFqsj0iQZm"},"source":["# Q[0][0] & K[0][3]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"f0PQ5AyAifES"},"source":["# Q[0][0] & K[0][*]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-MFt5KvtizL7"},"source":["# Q[0][*] & K[0][*]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"99yg9lw_k8AT"},"source":["# attention score"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qdlBCEbzlIqN"},"source":["# attention prob"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1MP4LSBtlPfY"},"source":["# attn_prov[0][0][0] & V[0][0]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GgO7FjFwl9sB"},"source":["# attn_prov[0][0][1] & V[0][1]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Hd_qEqVRmGcx"},"source":["# attn_prov[0][0][2] & V[0][2]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"q3bMneusmL5C"},"source":["# attn_prov[0][0][3] & V[0][3]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GT_QmsqrmUxB"},"source":["# attn_prov[0][0][*] & V[0][*]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZLC8443Dpvr1"},"source":["# attn_prov[0][0][*] & V[0][*]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uXu-zatbqXkB"},"source":["# attn_prov[0][*] & V[0][*]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rgcaDOmeq3CP"},"source":["# attn_prov & V"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZqJWz2uIYPiQ"},"source":["class DotProductAttention(tf.keras.layers.Layer):\n","    def __init__(self, **kwargs):\n","        super().__init__(**kwargs)\n","\n","    def call(self, inputs):\n","        Q, K, V = inputs\n","        #####################################\n","        #####################################"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5rsUvTC6Y4hM"},"source":["attention = DotProductAttention()\n","attention((Q, K, V))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"D-bjMMvkBRLU"},"source":["# 모델링"]},{"cell_type":"code","metadata":{"id":"9h-hm9yHsDbr"},"source":["# word embedding\n","hidden_enc = embedding(train_enc_inputs)  # (bs, n_enc_seq, 5)\n","hidden_dec = embedding(train_dec_inputs)  # (bs, n_dec_seq, 5)\n","hidden_enc, hidden_dec"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0sPE6mN6rjVI"},"source":["# encoder lstm\n","fw_cell = tf.keras.layers.LSTM(units=4, return_state=True)\n","bw_cell = tf.keras.layers.LSTM(units=4, go_backwards=True, return_state=True)\n","lstm_enc = tf.keras.layers.Bidirectional(fw_cell, backward_layer=bw_cell)\n","hidden_enc, fw_h, fw_c, bw_h, bw_c = lstm_enc(hidden_enc)  # (bs, d_model * 2), (bs, d_model), (bs, d_model), (bs, d_model), (bs, d_model)\n","hidden_enc.shape, fw_h.shape, fw_c.shape, bw_h.shape, bw_c.shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WjINyjN_ryYS"},"source":["# concat hidden & cell\n","s_h = tf.concat([fw_h, bw_h], axis=-1)  # (bs, d_model * 2)\n","s_c = tf.concat([fw_c, bw_c], axis=-1)  # (bs, d_model * 2)\n","s_h.shape, s_c.shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Jm8UyN8hr2HR"},"source":["# decoder LSTM\n","lstm_dec = tf.keras.layers.LSTM(units=8, return_sequences=True)\n","hidden_dec = lstm_dec(hidden_dec, initial_state=[s_h, s_c])  # (bs, n_dec_seq, d_model)\n","hidden_dec.shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"o2MF4uCHsX8L"},"source":["# attention"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"poyo-V79spZD"},"source":["# concat output"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3vEkJI5ossNJ"},"source":["# 다음단어 예측\n","dense_out = tf.keras.layers.Dense(units=len(word_to_id), activation=tf.nn.softmax)\n","outputs = dense_out(dec_out)  # (bs, n_dec_seq, n_vocab)\n","outputs.shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"in7q0WMfm64D"},"source":["def build_model(n_vocab, d_model, n_enc_seq, n_dec_seq):\n","    \"\"\"\n","    seq2seq 모델\n","    :param n_vocab: vocabulary 단어 수\n","    :param d_model: 단어를 의미하는 벡터의 차원 수\n","    :param n_enc_seq: encoder 문장 길이 (단어 수)\n","    :param n_dec_seq: decoder 문장 길이 (단어 수)\n","    \"\"\"\n","    inputs_enc = tf.keras.layers.Input((n_enc_seq,))  # (bs, n_enc_seq)\n","    inputs_dec = tf.keras.layers.Input((n_dec_seq,))  # (bs, n_dec_seq)\n","    ################################################\n","    # 입력 단어를 vector로 변환\n","    embedding = tf.keras.layers.Embedding(n_vocab, d_model)\n","    hidden_enc = embedding(inputs_enc)  # (bs, n_enc_seq, d_model)\n","    hidden_dec = embedding(inputs_dec)  # (bs, n_dec_seq, d_model)\n","\n","    # Encoder LSTM (uni-direction, bi-direction 가능)\n","    fw_cell = tf.keras.layers.LSTM(units=d_model, return_state=True)\n","    bw_cell = tf.keras.layers.LSTM(units=d_model, go_backwards=True, return_state=True)\n","    lstm_enc = tf.keras.layers.Bidirectional(fw_cell, backward_layer=bw_cell)\n","    hidden_enc, fw_h, fw_c, bw_h, bw_c = lstm_enc(hidden_enc)  # (bs, d_model * 2), (bs, d_model), (bs, d_model), (bs, d_model), (bs, d_model)\n","\n","    # Concatenate hidden states and cell states\n","    s_h = K.concatenate([fw_h, bw_h], axis=-1)  # (bs, d_model * 2)\n","    s_c = K.concatenate([fw_c, bw_c], axis=-1)  # (bs, d_model * 2)\n","\n","    # Decoder LSTM (uni-direction만 가능)\n","    lstm_dec = tf.keras.layers.LSTM(units=d_model * 2, return_sequences=True)\n","    hidden_dec = lstm_dec(hidden_dec, initial_state=[s_h, s_c])  # (bs, n_dec_seq, d_model)\n","    \n","    # 다음단어 예측\n","    dense_out = tf.keras.layers.Dense(units=n_vocab, activation=tf.nn.softmax)\n","    outputs = dense_out(hidden_dec)  # (bs, n_dec_seq, n_vocab)\n","    ################################################\n","    # 학습할 모델 선언\n","    model = tf.keras.Model(inputs=(inputs_enc, inputs_dec), outputs=outputs)\n","    return model"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"H8Bkh2pYB8zp"},"source":["# 모델 생성\n","model = build_model(len(vocab), d_model, n_enc_seq, n_dec_seq)\n","# 모델 내용 그래프 출력\n","tf.keras.utils.plot_model(model, 'model.png', show_shapes=True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XwriCkq_R1Lc"},"source":["# Vocabulary & config"]},{"cell_type":"code","metadata":{"id":"2H0BLydCb7lg"},"source":["# vocab loading\n","vocab = spm.SentencePieceProcessor()\n","vocab.load(os.path.join(data_dir, 'ko_32000.model'))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0ETZ19flTRmt"},"source":["n_vocab = len(vocab)  # number of vocabulary\n","n_enc_seq = 32  # number of sequence 1\n","n_dec_seq = 40  # number of sequence 2\n","d_model = 256  # dimension of model"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kTgzpx2Wl6uv"},"source":["# Loss & Acc"]},{"cell_type":"code","metadata":{"id":"hQRKhodYtQHr"},"source":["def lm_loss(y_true, y_pred):\n","    \"\"\"\n","    pad 부분을 제외하고 loss를 계산하는 함수\n","    :param y_true: 정답\n","    :param y_pred: 예측 값\n","    :retrun loss: pad 부분이 제외된 loss 값\n","    \"\"\"\n","    # loss = sparse_entropy = tf.keras.losses.SparseCategoricalCrossentropy()(y_true, y_pred)\n","    loss = tf.keras.losses.SparseCategoricalCrossentropy(reduction=tf.keras.losses.Reduction.NONE)(y_true, y_pred)\n","    mask = tf.not_equal(y_true, 0)\n","    mask = tf.cast(mask, tf.float32)\n","    # print(mask)\n","    loss *= mask\n","    return loss"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xQqs7xzA3N3Q"},"source":["def lm_acc(y_true, y_pred):\n","    \"\"\"\n","    pad 부분을 제외하고 accuracy를 계산하는 함수\n","    :param y_true: 정답\n","    :param y_pred: 예측 값\n","    :retrun loss: pad 부분이 제외된 accuracy 값\n","    \"\"\"\n","    y_true = tf.cast(y_true, tf.float32)\n","    # print(y_true)\n","    y_pred_class = tf.cast(tf.argmax(y_pred, axis=-1), tf.float32)\n","    # print(y_pred_class)\n","    matches = tf.cast(tf.equal(y_true, y_pred_class), tf.float32)\n","    # print(matches)\n","    mask = tf.not_equal(y_true, 0)\n","    mask = tf.cast(mask, tf.float32)\n","    # print(mask)\n","    matches *= mask\n","    # print(matches)\n","    # accuracy = tf.reduce_sum(matches) / tf.maximum(tf.reduce_sum(tf.ones_like(matches)), 1)\n","    accuracy = tf.reduce_sum(matches) / tf.maximum(tf.reduce_sum(mask), 1)\n","    return accuracy"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gcG7zBAr_R_4"},"source":["# Sample Data Project"]},{"cell_type":"markdown","metadata":{"id":"jv0OPyaKSpjw"},"source":["## Data\n"]},{"cell_type":"code","metadata":{"id":"98u39IsbSoBk"},"source":["df_train = pd.read_csv(os.path.join(songys_dir, 'ChatbotData.csv'))\n","df_train"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FPVe-1M7tJyO"},"source":["df_train = df_train.dropna()\n","df_train"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lAZD9iR6_oO_"},"source":["# 랜덤하게 10개만 확인\n","df_train = df_train.sample(10)\n","df_train"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pkzwlvcVS1cj"},"source":["def load_data(df, n_enc_seq, n_dec_seq):\n","    \"\"\"\n","    Quora 학습 데이터 생성\n","    :param df: data frame\n","    :param n_enc_seq: number of encoder sequence\n","    :param n_dec_seq: number of decoder sequence\n","    :return enc_inputs: encoder input data\n","    :return dec_inputs: decoder input data\n","    :return labels: label data\n","    \"\"\"\n","    n_enc_max = n_enc_seq\n","    n_dec_max = n_dec_seq - 1\n","    enc_inputs = np.zeros((len(df), n_enc_seq)).astype(np.int32)\n","    dec_inputs = np.zeros((len(df), n_dec_seq)).astype(np.int32)\n","    labels = np.zeros((len(df), n_dec_seq)).astype(np.int32)\n","    index = 0\n","    for i, row in tqdm(df.iterrows(), total=len(df)):\n","        # tokens 저장\n","        print()\n","        Q = row['Q']\n","        A = row['A']\n","        print(Q, '/', A)\n","\n","        tokens_q = vocab.encode_as_pieces(Q)\n","        print(len(tokens_q), ':', tokens_q)\n","        tokens_a = vocab.encode_as_pieces(A)\n","        print(len(tokens_a), ':', tokens_a)\n","\n","        tokens_ids_q = vocab.encode_as_ids(Q)[:n_enc_max]\n","        print(len(tokens_ids_q), ':', tokens_ids_q)\n","        tokens_ids_a = vocab.encode_as_ids(A)[:n_dec_max]\n","        print(len(tokens_ids_a), ':', tokens_ids_a)\n","\n","        tokens_dec_in = [vocab.bos_id()] + tokens_ids_a\n","        tokens_dec_out = tokens_ids_a + [vocab.eos_id()]\n","\n","        tokens_ids_q += [0] * (n_enc_seq - len(tokens_ids_q))\n","        print(len(tokens_ids_q), ':', tokens_ids_q)\n","        tokens_dec_in += [0] * (n_dec_seq - len(tokens_dec_in))\n","        print(len(tokens_dec_in), ':', tokens_dec_in)\n","        tokens_dec_out += [0] * (n_dec_seq - len(tokens_dec_out))\n","        print(len(tokens_dec_out), ':', tokens_dec_out)\n","\n","        enc_inputs[index] = tokens_ids_q\n","        dec_inputs[index] = tokens_dec_in\n","        labels[index] = tokens_dec_out\n","        index += 1\n","    return enc_inputs, dec_inputs, labels"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bfEwG-jgS4aN"},"source":["# train data 생성\n","train_enc_inputs, train_dec_inputs, train_labels = load_data(df_train, n_enc_seq, n_dec_seq)\n","train_enc_inputs, train_dec_inputs, train_labels"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IFTsdA3kE2h-"},"source":["## 학습"]},{"cell_type":"code","metadata":{"id":"oTbV-P7IpsLH"},"source":["# 모델 생성\n","model = build_model(len(vocab), d_model, n_enc_seq, n_dec_seq)\n","# 모델 내용 그래프 출력\n","tf.keras.utils.plot_model(model, 'model.png', show_shapes=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"X7UDvaPqEx1c"},"source":["# 모델 loss, optimizer, metric 정의\n","model.compile(loss=lm_loss, optimizer='adam', metrics=[lm_acc])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-FQBJ2aQLXdp"},"source":["# early stopping\n","early_stopping = tf.keras.callbacks.EarlyStopping(monitor='lm_acc', patience=100)\n","# save weights callback\n","save_weights = tf.keras.callbacks.ModelCheckpoint(os.path.join(songys_dir, 'lstm_dot.hdf5'),\n","                                                  monitor='lm_acc',\n","                                                  verbose=1,\n","                                                  save_best_only=True,\n","                                                  mode=\"max\",\n","                                                  save_freq=\"epoch\",\n","                                                  save_weights_only=True)\n","# csv logger\n","csv_logger = tf.keras.callbacks.CSVLogger(os.path.join(songys_dir, 'lstm_dot.csv'))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MSyF0MvAFGoJ"},"source":["# 모델 학습\n","history = model.fit((train_enc_inputs, train_dec_inputs),\n","                    train_labels,\n","                    epochs=400,\n","                    batch_size=256,\n","                    callbacks=[early_stopping, save_weights, csv_logger])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"m1UWJMf4FTiZ"},"source":["plt.figure(figsize=(12, 4))\n","\n","plt.subplot(1, 2, 1)\n","plt.plot(history.history['loss'], 'b-', label='loss')\n","plt.xlabel('Epoch')\n","plt.legend()\n","\n","plt.subplot(1, 2, 2)\n","plt.plot(history.history['lm_acc'], 'g-', label='acc')\n","plt.xlabel('Epoch')\n","plt.legend()\n","\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PQvKAxTbaEpt"},"source":["## Inference"]},{"cell_type":"code","metadata":{"id":"ZuK60XkEOSUS"},"source":["# 모델 생성\n","model = build_model(len(vocab), d_model, n_enc_seq, n_dec_seq)\n","# train weight로 초기화\n","model.load_weights(os.path.join(songys_dir, 'lstm_dot.hdf5'))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KzaDEUSbaeu4"},"source":["def do_chat(vocab, model, n_enc_seq, n_dec_seq, string):\n","    \"\"\"\n","    seq2seq chat\n","    :param vocab: vocab\n","    :param model: model\n","    :param n_enc_seq: number of enc seqence\n","    :param n_dec_seq: number of dec seqence\n","    :param string: inpust string\n","    \"\"\"\n","    # qeustion\n","    q = vocab.encode_as_pieces(string)\n","    q_id = [vocab.piece_to_id(p) for p in q][:n_enc_seq]\n","    q_id += [0] * (n_enc_seq - len(q_id))\n","    assert len(q_id) == n_enc_seq\n","\n","    # answer\n","    a_id = [vocab.bos_id()]\n","    a_id += [0] * (n_dec_seq - len(a_id))\n","    assert len(a_id) == n_dec_seq\n","\n","    # 처음부터 예측\n","    start_idx = 0\n","\n","    for _ in range(start_idx, n_dec_seq - 1):\n","        outputs = model.predict((np.array([q_id]), np.array([a_id])))\n","        prob = outputs[0][start_idx]\n","        word_id = np.argmax(prob)\n","        if word_id == vocab.eos_id():\n","            break\n","        a_id[start_idx + 1] = int(word_id)\n","        start_idx += 1\n","    predict_id = a_id[1:start_idx + 1]\n","    predict_str = vocab.decode_ids(predict_id)\n","    return predict_str"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mQVkuBylavCr"},"source":["while True:\n","    string = input('질문 > ')\n","    string = string.strip()\n","    if len(string) == 0:\n","        break\n","    predict_str = do_chat(vocab, model, n_enc_seq, n_dec_seq, string)\n","    print(f'답변 > {predict_str}')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wZSehzviqJW3"},"source":["# All Data Project"]},{"cell_type":"markdown","metadata":{"id":"x0ZAlao5qJW4"},"source":["## Data\n"]},{"cell_type":"code","metadata":{"id":"0B-0r3bjqJW4"},"source":["df_train = pd.read_csv(os.path.join(songys_dir, 'ChatbotData.csv'))\n","print(len(df_train))\n","df_train = df_train.dropna()\n","print(len(df_train))\n","df_train"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SQkIYiRSqJW5"},"source":["def load_data(df, n_enc_seq, n_dec_seq):\n","    \"\"\"\n","    Quora 학습 데이터 생성\n","    :param df: data frame\n","    :param n_enc_seq: number of encoder sequence\n","    :param n_dec_seq: number of decoder sequence\n","    :return enc_inputs: encoder input data\n","    :return dec_inputs: decoder input data\n","    :return labels: label data\n","    \"\"\"\n","    n_enc_max = n_enc_seq\n","    n_dec_max = n_dec_seq - 1\n","    enc_inputs = np.zeros((len(df), n_enc_seq)).astype(np.int32)\n","    dec_inputs = np.zeros((len(df), n_dec_seq)).astype(np.int32)\n","    labels = np.zeros((len(df), n_dec_seq)).astype(np.int32)\n","    index = 0\n","    for i, row in tqdm(df.iterrows(), total=len(df)):\n","        # tokens 저장\n","        Q = row['Q']\n","        A = row['A']\n","\n","        tokens_q = vocab.encode_as_pieces(Q)\n","        tokens_a = vocab.encode_as_pieces(A)\n","\n","        tokens_ids_q = vocab.encode_as_ids(Q)[:n_enc_max]\n","        tokens_ids_a = vocab.encode_as_ids(A)[:n_dec_max]\n","\n","        tokens_dec_in = [vocab.bos_id()] + tokens_ids_a\n","        tokens_dec_out = tokens_ids_a + [vocab.eos_id()]\n","\n","        tokens_ids_q += [0] * (n_enc_seq - len(tokens_ids_q))\n","        tokens_dec_in += [0] * (n_dec_seq - len(tokens_dec_in))\n","        tokens_dec_out += [0] * (n_dec_seq - len(tokens_dec_out))\n","\n","        enc_inputs[index] = tokens_ids_q\n","        dec_inputs[index] = tokens_dec_in\n","        labels[index] = tokens_dec_out\n","        index += 1\n","    return enc_inputs, dec_inputs, labels"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FguUxO5xqJW5"},"source":["# train data 생성\n","train_enc_inputs, train_dec_inputs, train_labels = load_data(df_train, n_enc_seq, n_dec_seq)\n","train_enc_inputs, train_dec_inputs, train_labels"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ByyA-3bbqJW6"},"source":["## 학습"]},{"cell_type":"code","metadata":{"id":"Li7XjkxHqJW6"},"source":["# 모델 생성\n","model = build_model(len(vocab), d_model, n_enc_seq, n_dec_seq)\n","# 모델 내용 그래프 출력\n","tf.keras.utils.plot_model(model, 'model.png', show_shapes=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yRh1IFPCqJW6"},"source":["# 모델 loss, optimizer, metric 정의\n","model.compile(loss=lm_loss, optimizer='adam', metrics=[lm_acc])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"s5CzpAnaqJW6"},"source":["# early stopping\n","early_stopping = tf.keras.callbacks.EarlyStopping(monitor='lm_acc', patience=5)\n","# save weights callback\n","save_weights = tf.keras.callbacks.ModelCheckpoint(os.path.join(songys_dir, 'lstm_dot.hdf5'),\n","                                                  monitor='lm_acc',\n","                                                  verbose=1,\n","                                                  save_best_only=True,\n","                                                  mode=\"max\",\n","                                                  save_freq=\"epoch\",\n","                                                  save_weights_only=True)\n","# csv logger\n","csv_logger = tf.keras.callbacks.CSVLogger(os.path.join(songys_dir, 'lstm_dot.csv'))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9q0mkUKuqJW6"},"source":["# 모델 학습\n","history = model.fit((train_enc_inputs, train_dec_inputs),\n","                    train_labels,\n","                    epochs=100,\n","                    batch_size=256,\n","                    callbacks=[early_stopping, save_weights, csv_logger])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6PMeESUYqJW6"},"source":["plt.figure(figsize=(12, 4))\n","\n","plt.subplot(1, 2, 1)\n","plt.plot(history.history['loss'], 'b-', label='loss')\n","plt.xlabel('Epoch')\n","plt.legend()\n","\n","plt.subplot(1, 2, 2)\n","plt.plot(history.history['lm_acc'], 'g-', label='acc')\n","plt.xlabel('Epoch')\n","plt.legend()\n","\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2lWovHbAqJW7"},"source":["## Inference"]},{"cell_type":"code","metadata":{"id":"V11xf5bUqJW7"},"source":["# 모델 생성\n","model = build_model(len(vocab), d_model, n_enc_seq, n_dec_seq)\n","# train weight로 초기화\n","model.load_weights(os.path.join(songys_dir, 'lstm_dot.hdf5'))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rqMWscTSqJW7"},"source":["def do_chat(vocab, model, n_enc_seq, n_dec_seq, string):\n","    \"\"\"\n","    seq2seq chat\n","    :param vocab: vocab\n","    :param model: model\n","    :param n_enc_seq: number of enc seqence\n","    :param n_dec_seq: number of dec seqence\n","    :param string: inpust string\n","    \"\"\"\n","    # qeustion\n","    q = vocab.encode_as_pieces(string)\n","    q_id = [vocab.piece_to_id(p) for p in q][:n_enc_seq]\n","    q_id += [0] * (n_enc_seq - len(q_id))\n","    assert len(q_id) == n_enc_seq\n","\n","    # answer\n","    a_id = [vocab.bos_id()]\n","    a_id += [0] * (n_dec_seq - len(a_id))\n","    assert len(a_id) == n_dec_seq\n","\n","    # 처음부터 예측\n","    start_idx = 0\n","\n","    for _ in range(start_idx, n_dec_seq - 1):\n","        outputs = model.predict((np.array([q_id]), np.array([a_id])))\n","        prob = outputs[0][start_idx]\n","        word_id = np.argmax(prob)\n","        if word_id == vocab.eos_id():\n","            break\n","        a_id[start_idx + 1] = int(word_id)\n","        start_idx += 1\n","    predict_id = a_id[1:start_idx + 1]\n","    predict_str = vocab.decode_ids(predict_id)\n","    return predict_str"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LiEbtt7JqJW7"},"source":["while True:\n","    string = input('질문 > ')\n","    string = string.strip()\n","    if len(string) == 0:\n","        break\n","    predict_str = do_chat(vocab, model, n_enc_seq, n_dec_seq, string)\n","    print(f'답변 > {predict_str}')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Yo6cyrr2ehCH"},"source":[""],"execution_count":null,"outputs":[]}]}